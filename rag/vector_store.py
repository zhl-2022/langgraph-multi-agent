# rag/vector_store.py
from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility
from transformers import AutoModel, AutoTokenizer
import torch
import numpy as np
from config import Config

class QwenEmbeddingModel:
    """Qwen3-Embeddingæ¨¡å‹å°è£…"""
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½Qwen3-Embeddingæ¨¡å‹"""
        print("ğŸ”„ åŠ è½½Qwen3-Embeddingæ¨¡å‹...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        self.model = AutoModel.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ… Qwen3-Embeddingæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _mean_pooling(self, model_output, attention_mask):
        """å‡å€¼æ± åŒ–ç­–ç•¥"""
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return sum_embeddings / sum_mask
    
    def encode(self, texts: list):
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
        if isinstance(texts, str):
            texts = [texts]
        
        try:
            # ç¼–ç æ–‡æœ¬
            inputs = self.tokenizer(
                texts, 
                padding=True, 
                truncation=True, 
                max_length=512, 
                return_tensors="pt"
            ).to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model(**inputs)
            
            # ä½¿ç”¨å‡å€¼æ± åŒ–è·å¾—æ–‡æ¡£çº§åµŒå…¥
            embeddings = self._mean_pooling(outputs, inputs['attention_mask'])
            
            # å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼Œä½†é€šå¸¸èƒ½æå‡æ£€ç´¢æ•ˆæœï¼‰
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
            
            return embeddings.cpu().numpy()
            
        except Exception as e:
            print(f"âŒ ç¼–ç å¤±è´¥: {e}")
            # è¿”å›éšæœºå‘é‡ä½œä¸ºå¤‡é€‰
            return np.random.randn(len(texts), 1024).astype(np.float32)

class MilvusVectorStore:
    def __init__(self, config: Config):
        self.config = config
        self.embedding_model = QwenEmbeddingModel(config.EMBEDDING_MODEL_PATH)
        self.collection = None
        self._connect()
        
    def _connect(self):
        """è¿æ¥Milvusæ•°æ®åº“"""
        try:
            connections.connect(
                alias="default",
                uri=self.config.MILVUS_URI,
                token=self.config.MILVUS_TOKEN,
                user=self.config.MILVUS_USER, 
                password=self.config.MILVUS_PASSWORD,
                secure=True
            )
            print("âœ… æˆåŠŸè¿æ¥åˆ°Milvusæ•°æ®åº“")
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            if utility.has_collection(self.config.COLLECTION_NAME):
                self.collection = Collection(self.config.COLLECTION_NAME)
                self.collection.load()
                print(f"âœ… é›†åˆ {self.config.COLLECTION_NAME} å·²å­˜åœ¨å¹¶å·²åŠ è½½")
            else:
                print(f"âš ï¸ é›†åˆ {self.config.COLLECTION_NAME} ä¸å­˜åœ¨ï¼Œå°†åœ¨éœ€è¦æ—¶åˆ›å»º")
                
        except Exception as e:
            print(f"âŒ è¿æ¥Milvusæ•°æ®åº“å¤±è´¥: {e}")
            self.collection = None
    
    def create_collection(self):
        """åˆ›å»ºå‘é‡é›†åˆ"""
        try:
            # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if utility.has_collection(self.config.COLLECTION_NAME):
                utility.drop_collection(self.config.COLLECTION_NAME)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§é›†åˆ: {self.config.COLLECTION_NAME}")
            
            # æµ‹è¯•è·å–ç»´åº¦
            test_embedding = self.embedding_model.encode(["æµ‹è¯•æ–‡æœ¬"])
            embedding_dim = test_embedding.shape[1]
            print(f"ğŸ“ Qwen3-Embeddingç»´åº¦: {embedding_dim}")
            
            # ä½¿ç”¨æ­£ç¡®çš„ç»´åº¦å®šä¹‰å­—æ®µ
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
                FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim),
                FieldSchema(name="metadata", dtype=DataType.JSON)
            ]
            
            schema = CollectionSchema(fields, "ä¼ä¸šçŸ¥è¯†åº“å‘é‡å­˜å‚¨")
            self.collection = Collection(self.config.COLLECTION_NAME, schema)
            
            # åˆ›å»ºå‘é‡ç´¢å¼•
            index_params = {
                "index_type": "AUTOINDEX",
                "metric_type": "L2", 
                "params": {}
            }
            self.collection.create_index("embedding", index_params)
            self.collection.load()
            
            print(f"âœ… æˆåŠŸåˆ›å»ºé›†åˆ: {self.config.COLLECTION_NAME} (ç»´åº¦: {embedding_dim})")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            raise
    
    def add_documents(self, documents: list, metadatas: list = None):
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“"""
        if self.collection is None:
            print("âŒ é›†åˆæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆ›å»ºé›†åˆ")
            return False
            
        if metadatas is None:
            metadatas = [{}] * len(documents)
        
        try:
            # ç”ŸæˆåµŒå…¥å‘é‡
            print("ğŸ”„ ä½¿ç”¨Qwen3-Embeddingç”ŸæˆåµŒå…¥å‘é‡...")
            embeddings = self.embedding_model.encode(documents)
            
            print(f"ğŸ“ åµŒå…¥çŸ©é˜µå½¢çŠ¶: {embeddings.shape}")
            print(f"ğŸ“ å®é™…åµŒå…¥ç»´åº¦: {embeddings.shape[1]}")
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            embeddings_list = embeddings.tolist()
            
            # å‡†å¤‡æ’å…¥æ•°æ®
            entities = [
                documents,  # contentå­—æ®µ
                embeddings_list, # embeddingå­—æ®µ  
                metadatas  # metadataå­—æ®µ
            ]
            
            # æ’å…¥æ•°æ®
            print("ğŸ”„ æ’å…¥æ•°æ®åˆ°Milvus...")
            insert_result = self.collection.insert(entities)
            self.collection.flush()
            
            print(f"âœ… æˆåŠŸæ’å…¥ {len(documents)} ä¸ªæ–‡æ¡£")
            print(f"ğŸ“ˆ é›†åˆç°åœ¨æœ‰ {self.collection.num_entities} ä¸ªå®ä½“")
            return True
            
        except Exception as e:
            print(f"âŒ æ’å…¥æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def similarity_search(self, query: str, k: int = 5):
        """ç›¸ä¼¼æ€§æœç´¢"""
        if self.collection is None:
            print("âŒ é›†åˆæœªåˆå§‹åŒ–")
            return []
            
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode([query])
            query_embedding_list = query_embedding.tolist()
            
            print(f"ğŸ” æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_embedding_list[0])}")
            
            # æ‰§è¡Œæœç´¢
            search_params = {"metric_type": "L2", "params": {"ef": 32}}
            
            results = self.collection.search(
                data=query_embedding_list,
                anns_field="embedding",
                param=search_params,
                limit=k,
                output_fields=["content", "metadata"]
            )
            
            search_results = []
            for hit in results[0]:
                search_results.append({
                    'content': hit.entity.get('content'),
                    'metadata': hit.entity.get('metadata', {}),
                    'distance': hit.distance
                })
            
            print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªç»“æœ")
            return search_results
            
        except Exception as e:
            print(f"âŒ æœç´¢è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return []

    def get_collection_info(self):
        """è·å–é›†åˆä¿¡æ¯"""
        if self.collection is None:
            return "é›†åˆæœªåˆå§‹åŒ–"
        
        try:
            num_entities = self.collection.num_entities
            return f"é›†åˆ: {self.config.COLLECTION_NAME}, å®ä½“æ•°é‡: {num_entities}"
        except:
            return f"é›†åˆ: {self.config.COLLECTION_NAME}, çŠ¶æ€: å·²åŠ è½½"