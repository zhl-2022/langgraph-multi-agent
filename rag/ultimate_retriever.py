# rag/ultimate_retriever.py
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from typing import List, Dict
from config import Config
import os

class UltimateReranker:
    """ç»ˆæç¨³å®šç‰ˆReranker - å½»åº•è§£å†³paddingé—®é¢˜"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self._load_model_safely()
    
    def _load_model_safely(self):
        """å®‰å…¨åŠ è½½æ¨¡å‹"""
        try:
            print(f"ğŸ”„ åŠ è½½Rerankeræ¨¡å‹ä»: {self.model_path}")
            
            # æ–¹æ³•1: å°è¯•ç›´æ¥åŠ è½½
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    trust_remote_code=True
                )
            except Exception as e:
                print(f"âš ï¸ æ ‡å‡†åŠ è½½å¤±è´¥: {e}")
                # æ–¹æ³•2: ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åŠ è½½
                self._load_from_local_files()
            
            # å¼ºåˆ¶è®¾ç½®paddingé…ç½®
            self._force_padding_config()
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            print("âœ… Rerankeræ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥: {e}")
            self.model = None
            self.tokenizer = None
    
    def _load_from_local_files(self):
        """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½tokenizer"""
        try:
            # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
            required_files = ['tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json']
            has_files = all(os.path.exists(os.path.join(self.model_path, f)) for f in required_files)
            
            if has_files:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path,
                    local_files_only=True,
                    trust_remote_code=True
                )
                print("âœ… ä»æœ¬åœ°æ–‡ä»¶åŠ è½½tokenizeræˆåŠŸ")
            else:
                raise FileNotFoundError("ç¼ºå°‘å¿…è¦çš„tokenizeræ–‡ä»¶")
                
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _force_padding_config(self):
        """å¼ºåˆ¶è®¾ç½®paddingé…ç½®"""
        if self.tokenizer is None:
            return
            
        # ç¡®ä¿æœ‰pad_token
        if self.tokenizer.pad_token is None:
            if hasattr(self.tokenizer, 'eos_token') and self.tokenizer.eos_token is not None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            elif hasattr(self.tokenizer, 'unk_token') and self.tokenizer.unk_token is not None:
                self.tokenizer.pad_token = self.tokenizer.unk_token
            else:
                # æ·»åŠ æ–°çš„pad_token
                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            
            print(f"âœ… è®¾ç½®pad_tokenä¸º: {self.tokenizer.pad_token}")
        
        # ç¡®ä¿pad_token_idæœ‰æ•ˆ
        if self.tokenizer.pad_token_id is None:
            if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            else:
                self.tokenizer.pad_token_id = 0  # é»˜è®¤å€¼
        
        print(f"ğŸ“‹ æœ€ç»ˆé…ç½® - pad_token: {self.tokenizer.pad_token}, pad_token_id: {self.tokenizer.pad_token_id}")
    
    def rerank_ultra_safe(self, query: str, documents: List[str]) -> List[Dict]:
        """è¶…å®‰å…¨é‡æ’åº - å½»åº•é¿å…æ‰¹é‡é—®é¢˜"""
        if not documents or self.model is None or self.tokenizer is None:
            return self._create_default_results(documents)
        
        results = []
        
        for i, doc in enumerate(documents):
            try:
                score = self._score_single_pair(query, doc)
                results.append({
                    'document': doc,
                    'score': score,
                    'rank': i
                })
                
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡æ¡£ {i} å¤±è´¥: {e}")
                results.append({
                    'document': doc,
                    'score': 0.5,
                    'rank': i
                })
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results
    
    def _score_single_pair(self, query: str, document: str) -> float:
        """ä¸ºå•ä¸ªæŸ¥è¯¢-æ–‡æ¡£å¯¹è¯„åˆ†"""
        try:
            # æ„å»ºå•ä¸ªå¯¹
            text_pair = [query, document]
            
            # ä½¿ç”¨æœ€å®‰å…¨çš„ç¼–ç æ–¹å¼
            inputs = self.tokenizer(
                text_pair,
                padding='max_length',      # å›ºå®šé•¿åº¦padding
                truncation=True,
                max_length=256,           # è¾ƒçŸ­çš„åºåˆ—
                return_tensors="pt",
                return_attention_mask=True,
                return_token_type_ids=True
            )
            
            # æ‰‹åŠ¨æ£€æŸ¥å¹¶ä¿®å¤è¾“å…¥
            inputs = self._validate_and_fix_inputs(inputs)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                scores = torch.softmax(outputs.logits, dim=-1)
                score = scores[0, 1].item()  # æ­£ä¾‹åˆ†æ•°
            
            return score
            
        except Exception as e:
            print(f"âŒ è¯„åˆ†å¤±è´¥: {e}")
            return 0.5
    
    def _validate_and_fix_inputs(self, inputs):
        """éªŒè¯å’Œä¿®å¤è¾“å…¥"""
        # ç¡®ä¿attention_maskå­˜åœ¨
        if 'attention_mask' not in inputs:
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])
        
        # ç¡®ä¿token_type_idså­˜åœ¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if 'token_type_ids' not in inputs and hasattr(self.model.config, 'type_vocab_size'):
            seq_length = inputs['input_ids'].shape[1]
            inputs['token_type_ids'] = torch.zeros((1, seq_length), dtype=torch.long)
        
        return inputs
    
    def _create_default_results(self, documents: List[str]) -> List[Dict]:
        """åˆ›å»ºé»˜è®¤ç»“æœ"""
        return [{'document': doc, 'score': 0.5, 'rank': i} for i, doc in enumerate(documents)]

class UltimateRetriever:
    """ç»ˆææ£€ç´¢å™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        
        # åªæœ‰åœ¨è·¯å¾„æœ‰æ•ˆæ—¶æ‰åˆå§‹åŒ–reranker
        if (config.RERANKER_MODEL_PATH and 
            os.path.exists(config.RERANKER_MODEL_PATH) and
            self._check_model_validity(config.RERANKER_MODEL_PATH)):
            
            self.reranker = UltimateReranker(config.RERANKER_MODEL_PATH)
            print("âœ… ä½¿ç”¨ç»ˆæç‰ˆæ£€ç´¢å™¨")
        else:
            self.reranker = None
            print("âš ï¸ ä½¿ç”¨æ— Rerankerçš„ç®€åŒ–æ£€ç´¢")
    
    def _check_model_validity(self, model_path: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æœ‰æ•ˆæ€§"""
        try:
            # ç®€å•æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¿…è¦çš„æ–‡ä»¶
            required = ['config.json', 'pytorch_model.bin', 'model.safetensors']
            has_required = any(os.path.exists(os.path.join(model_path, f)) for f in required)
            return has_required
        except:
            return False
    
    def retrieve(self, query: str, vector_store, top_k: int = 5) -> List[Dict]:
        """æ£€ç´¢æ–¹æ³•"""
        try:
            if vector_store is None:
                return []
                
            # å‘é‡æ£€ç´¢
            vector_results = vector_store.similarity_search(query, k=top_k)
            
            if not vector_results or self.reranker is None or self.reranker.model is None:
                return vector_results[:top_k]
            
            # é‡æ’åº
            print("ğŸ”„ ä½¿ç”¨ç»ˆæç‰ˆRerankerè¿›è¡Œé‡æ’åº...")
            documents = [result['content'] for result in vector_results]
            reranked = self.reranker.rerank_ultra_safe(query, documents)
            
            # åˆå¹¶ç»“æœ
            final_results = []
            for item in reranked[:top_k]:
                idx = item['rank']
                if idx < len(vector_results):
                    final_result = vector_results[idx].copy()
                    final_result['rerank_score'] = item['score']
                    final_result['final_score'] = item['score']
                    final_results.append(final_result)
            
            print(f"âœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results
            
        except Exception as e:
            print(f"âŒ æ£€ç´¢å¤±è´¥: {e}")
            # è¿”å›åŸå§‹å‘é‡ç»“æœ
            return vector_results[:top_k] if 'vector_results' in locals() else []