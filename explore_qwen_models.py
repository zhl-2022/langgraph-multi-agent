# explore_qwen_models.py
from transformers import AutoModel, AutoTokenizer
import torch
from config import Config

def explore_qwen_embedding():
    """æ¢ç´¢Qwen3-Embeddingçš„æ­£ç¡®API"""
    config = Config()
    
    print("ğŸ” æ¢ç´¢Qwen3-Embedding API")
    print("=" * 50)
    
    try:
        # åŠ è½½æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(
            config.EMBEDDING_MODEL_PATH,
            trust_remote_code=True
        )
        model = AutoModel.from_pretrained(
            config.EMBEDDING_MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {type(model)}")
        print(f"ğŸ“‹ æ¨¡å‹ç±»: {model.__class__}")
        
        # æ£€æŸ¥æ¨¡å‹çš„æ–¹æ³•
        methods = [method for method in dir(model) if not method.startswith('_')]
        print(f"ğŸ“‹ æ¨¡å‹æ–¹æ³•: {methods[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªæ–¹æ³•
        
        # æµ‹è¯•æ–‡æœ¬
        texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬", "è¿™æ˜¯å¦ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"]
        
        # å°è¯•ä¸åŒçš„ç¼–ç æ–¹å¼
        print("\nğŸ” å°è¯•ç¼–ç æ–¹å¼...")
        
        # æ–¹å¼1: ç›´æ¥è°ƒç”¨æ¨¡å‹
        try:
            inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model(**inputs)
            print("âœ… æ–¹å¼1æˆåŠŸ - ç›´æ¥è°ƒç”¨æ¨¡å‹")
            print(f"ğŸ“ è¾“å‡ºç±»å‹: {type(outputs)}")
            if hasattr(outputs, 'last_hidden_state'):
                embeddings = outputs.last_hidden_state
                print(f"ğŸ“ åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
        except Exception as e:
            print(f"âŒ æ–¹å¼1å¤±è´¥: {e}")
        
        # æ–¹å¼2: æ£€æŸ¥æ˜¯å¦æœ‰encodeæ–¹æ³•
        try:
            if hasattr(model, 'encode'):
                embeddings = model.encode(texts)
                print("âœ… æ–¹å¼2æˆåŠŸ - ä½¿ç”¨encodeæ–¹æ³•")
                print(f"ğŸ“ åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
        except Exception as e:
            print(f"âŒ æ–¹å¼2å¤±è´¥: {e}")
            
        # æ–¹å¼3: æ£€æŸ¥æ˜¯å¦æœ‰get_text_embeddingsæ–¹æ³•
        try:
            if hasattr(model, 'get_text_embeddings'):
                embeddings = model.get_text_embeddings(texts)
                print("âœ… æ–¹å¼3æˆåŠŸ - ä½¿ç”¨get_text_embeddingsæ–¹æ³•")
                print(f"ğŸ“ åµŒå…¥å½¢çŠ¶: {embeddings.shape}")
        except Exception as e:
            print(f"âŒ æ–¹å¼3å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"âŒ æ¢ç´¢å¤±è´¥: {e}")

if __name__ == "__main__":
    explore_qwen_embedding()